{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unique-allah",
   "metadata": {
    "id": "unique-allah"
   },
   "source": [
    "# Workshop #3: Multivariable Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "korean-thanksgiving",
   "metadata": {
    "id": "korean-thanksgiving"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-impression",
   "metadata": {
    "id": "essential-impression"
   },
   "source": [
    "## Problem 1\n",
    "Find the critical points of the function $f(x, y) = x^2 + y^4 - 4xy$ by using first partial derivatives. Then use second partial derivatives to establish whether each critical point is a local minimum, a local maximum, or a saddle point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powerful-montgomery",
   "metadata": {
    "id": "powerful-montgomery"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical points are (0, 0) and (-2*sqrt(2), -sqrt(2))\n",
      "d = -16 < 0 and we conclude that (0, 0) is a saddle point\n",
      "d = 32 >0 and we conclude that (-2*sqrt(2), -sqrt(2)) is a local minimum\n",
      "d = 32 >0 and we conclude that (2*sqrt(2), sqrt(2)) is a local minimum\n"
     ]
    }
   ],
   "source": [
    "# Define the variables and the function\n",
    "x, y = sp.symbols('x y', real =True)\n",
    "f = sp.Function('f',real=True)\n",
    "F = sp.Function('F',real=True)\n",
    "\n",
    "f = x**2 + y**4 -4*x*y\n",
    "# Differentiate\n",
    "f_x = f.diff(x)\n",
    "f_y = f.diff(y)\n",
    "\n",
    "eq_1 = sp.Eq(f_x, 0)\n",
    "eq_2 = sp.Eq(f_y, 0)\n",
    "\n",
    "# Find critical points\n",
    "crit_pts = sp.solve((eq_1, eq_2), (x, y))\n",
    "\n",
    "# Classifying the critical points\n",
    "print(f'Critical points are {crit_pts[0]} and {crit_pts[1]}')\n",
    "\n",
    "# Getting the second derivatives\n",
    "f_xx = f.diff(x, 2)\n",
    "f_yy = f.diff(y, 2)\n",
    "f_xy = f.diff(x, y)\n",
    "\n",
    "\n",
    "# # Working with the first critical point\n",
    "dict_val = {x:crit_pts[0][0], y:crit_pts[0][1]}\n",
    "d = f_xx.subs(dict_val) * f_yy.subs(dict_val) - (f_xy.subs(dict_val))**2\n",
    "print(f'd = {d} < 0 and we conclude that {crit_pts[0]} is a saddle point')\n",
    "\n",
    "# Working with the second critical point\n",
    "dict_val2 = {x:crit_pts[1][0], y:crit_pts[1][1]}\n",
    "d2 = f_xx.subs(dict_val2) * f_yy.subs(dict_val2) - (f_xy.subs(dict_val2))**2\n",
    "print(f'd = {d2} >0 and we conclude that {crit_pts[1]} is a local minimum')\n",
    "\n",
    "# Working with the third critical point\n",
    "dict_val3 = {x:crit_pts[1][1], y:crit_pts[1][1]}\n",
    "d3 = f_xx.subs(dict_val3) * f_yy.subs(dict_val3) - (f_xy.subs(dict_val3))**2\n",
    "print(f'd = {d3} >0 and we conclude that {crit_pts[2]} is a local minimum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-newman",
   "metadata": {
    "id": "linear-newman"
   },
   "source": [
    "## Problem 2\n",
    "Using the **Gradient Descent Method** with initial approximation $\\mathbf{x}_0 = (0, 0)$, find the minimum point and the minimum value of the function $g(x, y) = (1 - x + x^2)\\cdot e^{y^2} + (1 - y + y^2)\\cdot e^{x^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certain-wrestling",
   "metadata": {
    "id": "certain-wrestling"
   },
   "outputs": [],
   "source": [
    "# Define variables and functions\n",
    "x, y = sp.symbols('x y')\n",
    "g = sp.Function('g',real=True)\n",
    "G = sp.Function('G',real=True)\n",
    "g = (1-x+x**2)*sp.exp(y**2)+ (1-y+y**2) * sp.exp(x**2)\n",
    "\n",
    "g_x = g.diff(x)\n",
    "g_y = g.diff(y)\n",
    "\n",
    "# Define the numpy functions\n",
    "g = sp.lambdify([x,y],g)\n",
    "g_x = sp.lambdify([x,y],g_x)\n",
    "g_y = sp.lambdify([x,y],g_y)\n",
    "\n",
    "def func(x_k):\n",
    "    x = x_k[0]\n",
    "    y = x_k[1]\n",
    "    return (g(x,y))\n",
    "\n",
    "def grad(x_k):\n",
    "    x = x_k[0]\n",
    "    y = x_k[1]\n",
    "\n",
    "    return np.array([g_x(x,y),\n",
    "                     g_y(x,y)])\n",
    "\n",
    "# x0 = np.array([0, 0])\n",
    "# gradient_descent(func, grad, x0, alpha=0.01)\n",
    "\n",
    "# Running the Gradient Descent Method\n",
    "def gradient_descent(g, grad, x0, alpha = 0.01, mode = 'min', max_iter = 1000, tol = 1e-6):\n",
    "    \n",
    "    # initialize the sequence\n",
    "    k = 0\n",
    "    xk = x0\n",
    "\n",
    "    if mode == 'max':\n",
    "        alpha = -alpha\n",
    "    while k < max_iter and np.linalg.norm(grad(xk),2) > tol: # loop until we do not have a solution\n",
    "\n",
    "        xk = xk - alpha * grad(xk) #calculate the new iteration\n",
    "        k = k + 1  #increment the iteration number\n",
    "      \n",
    "    return xk, g(xk), np.linalg.norm(grad(xk), 2), k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb635ea-52e5-46ba-86a2-857e770c1ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2778799, 0.2778799]), 1.7270110514248864, 9.643289486085021e-07, 387)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([0, 0])\n",
    "\n",
    "gradient_descent (func, grad, x0, max_iter=1100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-event",
   "metadata": {
    "id": "functioning-event"
   },
   "source": [
    "## Problem 3\n",
    "On a certain workday, the *rate*, in tons per hour, at which unprocessed gravel arrives at a gravel processing plant is modeled by $G(t) = 90 + 45\\cdot \\cos‚Å°\\left(\\frac{t^2}{18}\\right)$, where $t$ is measured in hours and $0 \\leqslant t \\leqslant 8$. At the beginning of the workday, $t = 0$, the plant has 500 tons of unprocessed gravel. During the hours of operation, $0 \\leqslant t \\leqslant 8$, the plant processes gravel at a constant rate $P(t) = 100$ tons per hour.\n",
    "* Find the total amount of unprocessed gravel that arrives at the plant during the hours of operation on this workday.\n",
    "* Is the amount of unprocessed gravel at the end of the workday (t=8) greater or smaller than amount of gravel at the beginning of the workday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "solid-choice",
   "metadata": {
    "id": "solid-choice"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*During the workday a total of 825.55 tons of gravel arrives at the plant\n",
      "*At the end of the workday, there will be 525.55 tons of unprocessed gravel at th eplant, which is more than the amount at the beginning of the workday\n"
     ]
    }
   ],
   "source": [
    "# Define variable and functions\n",
    "t = sp.symbols('t', positive=True)\n",
    "\n",
    "G = 90+45*sp.cos(t**2/18)\n",
    "P = 100\n",
    "\n",
    "# First bullet point\n",
    "ans_1 = sp.integrate(G, (t,0,8))\n",
    "print(f'*During the workday a total of {round(ans_1.evalf(),2)} tons of gravel arrives at the plant')\n",
    "\n",
    "# Second bullet point\n",
    "Q = 500 + sp.integrate(G, (t,0,8)) - sp.integrate (P, (t,0,8))\n",
    "print(f'*At the end of the workday, there will be {round(Q.evalf(),2)} tons of unprocessed gravel at th eplant, which is more than the amount at the beginning of the workday')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-start",
   "metadata": {
    "id": "destroyed-start"
   },
   "source": [
    "## Problem 4\n",
    "Solve the system of equations:\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "x - 2y + 3z &=& -1\\\\\n",
    "3x + 2y - 5z &=& 3\\\\\n",
    "2x - 5y + 2z &=& 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "using **Gradient Descent Method** applied to a function of the kind $f(x) = \\|A\\mathbf{x} - b\\|_2^2$ where $A\\mathbf{x} = b$ is the matrix equation that corresponds to the system, and $\\| \\cdot \\|_2$ is the Euclidean norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fallen-hours",
   "metadata": {
    "id": "fallen-hours"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.26086976, -0.08695635, -0.4782607 ]),\n",
       " 1.530250529911669e-13,\n",
       " 9.830335419305582e-07,\n",
       " 509)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables and matrices\n",
    "A = sp.Matrix([[1, -2, 3],\n",
    "               [3, 2, -5],\n",
    "               [2, -5, 2]])\n",
    "\n",
    "b = sp.Matrix([-1, 3, 0])\n",
    "\n",
    "x,y,z, X = sp.symbols('x y z X', real=True)\n",
    "\n",
    "X = sp.Matrix([x,y,z])\n",
    "\n",
    "# Define the function and derivatives\n",
    "f = ((A*X - b).norm())**2\n",
    "\n",
    "dfdx = sp.lambdify([x,y,z], f.diff(x))\n",
    "dfdy = sp.lambdify([x,y,z], f.diff(y))\n",
    "dfdz = sp.lambdify([x,y,z], f.diff(z))\n",
    "\n",
    "f = sp.lambdify([x,y,z],f)\n",
    "\n",
    "\n",
    "# Define the NumPy function\n",
    "\n",
    "def func(xk):\n",
    "    x = xk[0]\n",
    "    y = xk[1]\n",
    "    z = xk[2]\n",
    "\n",
    "    return f(x,y,z)\n",
    "\n",
    "def grad(xk):\n",
    "  x = xk[0]\n",
    "  y = xk[1]\n",
    "  z = xk[2]\n",
    "\n",
    "  return np.array([dfdx(x,y,z),\n",
    "                   dfdy(x,y,z),\n",
    "                   dfdz(x,y,z)])\n",
    "\n",
    "# Running the Gradient Descent Method\n",
    "xk = np.array([6,-1,1])\n",
    "\n",
    "gradient_descent(func, grad, xk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7pozFzUeQB1f",
   "metadata": {
    "id": "7pozFzUeQB1f"
   },
   "source": [
    "## Problem 5\n",
    "\n",
    "[Use Gradient Descent Method] Four people are typing two paragraphs of text. The first paragraph uses elementary vocabulary, while the second paragraph contains some more advanced words. The number of typos that the people make are labeled by ùë•, for the first paragraph, and ùë¶, for the second paragraph. Data collected is given in the accompanying table:\n",
    "\n",
    "| x \t | y \t|\n",
    "|---\t |---\t|\n",
    "| 1 \t | 4 \t|\n",
    "| 3 \t | 5 \t|\n",
    "| 4 \t | 6 \t|\n",
    "| 5 \t | 8 \t|\n",
    "\n",
    "\n",
    "We will build a model to predict the number of typos $y$ in the second text based on the number of typos in the first test. To do this, we use least-squares regression, an approach similar to the one we used for systems. For every number of observed typos $x_i$ for the first text, we have a corresponding number $y_i$ of observed typos for the second text. Our model $\\hat{y_i} = f(x_i)$ will make a prediction of the number of typos on the second text. Most often, the observed number of typos $y_i$ and the predicted number of typos $\\hat{y_i}$ will not be equal.\n",
    "Let us label each such discrepancy in values by $d_i$, in other words:\n",
    "\n",
    "\\begin{align}\n",
    "d_i = observed_i - predicted_i = y_i - \\hat{y_i}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We call these discrepancies residuals. The goal now is to choose a model which will minimize the total sum of the squares of the residuals, i.e. a model which will make the overall discrepancy between the observed data and the predictions based on it as small as possible. Note that the form of the residuals depends on the choice of the model. For example, choosing a linear model $\\hat{y_i}=a+bx$, we have prediction $\\hat{y_i}=a+bx$, so the residuals become:\n",
    "\n",
    "\\begin{align}\n",
    "d^2 = (y-\\hat{y})^2 = (y-a-bx)^2\n",
    "\\end{align}\n",
    "\n",
    "The total sum of the squares of the residuals is then $\\sum_{i=1}^n d_i^2 = \\sum_{i=1}^n (y_i-a-bx_i)^2$. This allows us to define the function $f$ that we need to minimize in order to build the model as:\n",
    "\n",
    "\\begin{align}\n",
    "f(a,b) = \\sum_{i=1}^n(y_i-a-bx_i)^2\n",
    "\\end{align}\n",
    "\n",
    "By minimizing this function, we find the values for the coefficients $a$ and $b$ in the model that minimize the discrepancy between the data and the type of model we chose to work with.\n",
    "\n",
    "*a)* Build a linear model for the data: $\\hat{y}=a+bx$. Then make a prediction about the number of typos $y$ a person would make when typing the second text if they have made $x=2$ typos when typing the first text. **Use your own gradient_descent function.**\n",
    "\n",
    "*b)* Build a quadratic model for the data: $\\hat{y}=a+bx+cx^2$. Then make a prediction about the same person from part a). **Use the minimizer BFGS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qCNOYmmxUjzO",
   "metadata": {
    "id": "qCNOYmmxUjzO"
   },
   "source": [
    "**a) Linear model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tDW7nTRgP-JD",
   "metadata": {
    "id": "tDW7nTRgP-JD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.68571354, 0.94285734]),\n",
       " 0.971428571428955,\n",
       " 9.946473346286155e-07,\n",
       " 1167)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables and matrices\n",
    "A = sp.Matrix([[1, 1], [1, 3], [1, 4], [1, 5]])\n",
    "y = sp.Matrix([4, 5, 6, 8])\n",
    "a, b, X = sp.symbols('a b X', real=True)\n",
    "X = sp.Matrix([a, b])\n",
    "\n",
    "# Define the function f\n",
    "f = sp.Function('f', real=True)\n",
    "f = ((y-A*X).norm())**2\n",
    "\n",
    "# Define the numpy function f\n",
    "dfda = sp.lambdify([a,b], f.diff(a))\n",
    "dfdb = sp.lambdify([a,b], f.diff(b))\n",
    "\n",
    "f = sp.lambdify([a,b], f)\n",
    "\n",
    "def func(xk):\n",
    "    a = xk[0]\n",
    "    b = xk[1]\n",
    "\n",
    "    return (f(a,b))\n",
    "\n",
    "def grad_f(xk):\n",
    "    a = xk[0]\n",
    "    b = xk[1] \n",
    "\n",
    "    return np.array([dfda(a,b),\n",
    "                   dfdb(a,b)])\n",
    "\n",
    "x0 = np.array([0, 2])\n",
    "gradient_descent(func, grad_f, x0, max_iter=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YzzxuAtIUsC6",
   "metadata": {
    "id": "YzzxuAtIUsC6"
   },
   "source": [
    "**b) Quadratic model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "E-7Kct6HUikW",
   "metadata": {
    "id": "E-7Kct6HUikW"
   },
   "outputs": [],
   "source": [
    "# Define variables and matrices\n",
    "A = sp.Matrix([[1, 1, 1], [1, 3, 9], [1, 4, 16], [1, 5, 25]])\n",
    "y = sp.Matrix([4, 5, 6, 8])\n",
    "a, b, c, X = sp.symbols('a b c X')\n",
    "X = sp.Matrix([a, b, c])\n",
    "\n",
    "# Define the function f\n",
    "f = ((y-A*X).norm())**2 \n",
    "f\n",
    "# Define the numpy function f\n",
    "dfda = sp.lambdify([a,b,c], f.diff(a))\n",
    "dfdb = sp.lambdify([a,b,c], f.diff(b))\n",
    "dfdc = sp.lambdify([a,b,c], f.diff(c))\n",
    "\n",
    "f = sp.lambdify([a,b,c], f)\n",
    "\n",
    "def func(xk):\n",
    "    a = xk[0]\n",
    "    b = xk[1]\n",
    "    c = xk[2]\n",
    "  \n",
    "\n",
    "    return (f(a,b,c))\n",
    "\n",
    "x0 = np.array([0, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2-_fEnpgUxaS",
   "metadata": {
    "id": "2-_fEnpgUxaS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 0.03636363636421916\n",
       " hess_inv: array([[ 2.28983046, -1.6420376 ,  0.24871486],\n",
       "       [-1.6420376 ,  1.41535663, -0.23194833],\n",
       "       [ 0.24871486, -0.23194833,  0.03961032]])\n",
       "      jac: array([9.40635800e-07, 1.73458830e-06, 9.85711813e-06])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 40\n",
       "      nit: 8\n",
       "     njev: 10\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 4.39999929, -0.65454448,  0.27272709])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the BFGS algorithm\n",
    "opt_output=opt.minimize(func, x0, method='BFGS')\n",
    "opt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfff6c9a-2444-482d-9b03-e2289c17b0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "≈∑ = 4.399999294713105-0.6545444759812679*x + 0.27272708723891087*x^2\n",
      "≈∑ = 4.399999294713105-0.6545444759812679*2 + 0.27272708723891087*x^2 = 4.181818691706213\n"
     ]
    }
   ],
   "source": [
    "x_y_z = opt_output.x\n",
    "print(f'≈∑ = {x_y_z[0]}{x_y_z[1]}*x + {x_y_z[2]}*x^2')\n",
    "x=2\n",
    "print(f'≈∑ = {x_y_z[0]}{x_y_z[1]}*{x} + {x_y_z[2]}*x^2 = {x_y_z[0] + x_y_z[1]*x + x_y_z[2]*x**2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449440f6-a910-4864-a26d-c2ea3a5fe552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Workshop #3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
